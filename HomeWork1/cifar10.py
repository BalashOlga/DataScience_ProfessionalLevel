import pandas as pd
import streamlit as st
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential, save_model, load_model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np
import time
import seaborn as sns
from PIL import Image, ImageEnhance
import io
import sqlite3
import json
import os
from datetime import datetime
import pickle
import joblib
import tempfile

# –û—á–∏—Å—Ç–∞ –∫—ç—à–∞ Streamlit –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.cache_data.clear()
st.cache_resource.clear()

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
PROJECT_FOLDERS = ['models', 'experiments', 'database']

def setup_project_folders():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
    created_folders = []
    existing_folders = []
    
    for folder in PROJECT_FOLDERS:
        try:
            os.makedirs(folder, exist_ok=True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –ø–∞–ø–∫–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∞
            if not os.listdir(folder):  # –ü–∞–ø–∫–∞ –ø—É—Å—Ç–∞—è
                created_folders.append(folder)
            else:
                existing_folders.append(folder)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–∞–ø–∫–∏ {folder}: {e}")
    
    return created_folders, existing_folders

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–ø–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞
created, existing = setup_project_folders()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="CIFAR-10 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä", layout="wide")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
SAVE_SETTINGS = {
    'knn': {
        'enabled': True,
        'max_size_mb': 10
    },
    'neural_network': {
        'enabled': True, 
        'max_size_mb': 50
    },
    'cnn': {
        'enabled': True,
        'max_size_mb': 200
    }
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
def get_model_size(model, model_type):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ MB"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
            if model_type in ['cnn', 'neural_network']:
                model.save(tmp.name)
            elif model_type == 'knn':
                with open(tmp.name, 'wb') as f:
                    pickle.dump(model, f)
            
            size_mb = os.path.getsize(tmp.name) / (1024 * 1024)
            os.unlink(tmp.name)
        
        return size_mb
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏: {e}")
        return 0

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
def save_model_with_check(model, filepath, model_type, experiment_name):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ —Ä–∞–∑–º–µ—Ä–∞"""
    
    settings = SAVE_SETTINGS.get(model_type, {'enabled': False, 'max_size_mb': 0})
    
    if not settings['enabled']:
        return False, f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π {model_type} –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
def get_model_size(model, model_type):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ MB —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤"""
    try:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–∞
        with tempfile.TemporaryDirectory() as temp_dir:
            if model_type in ['cnn', 'neural_network']:
                model_path = os.path.join(temp_dir, 'model.h5')
                model.save(model_path)
                size_mb = os.path.getsize(model_path) / (1024 * 1024)
            elif model_type == 'knn':
                model_path = os.path.join(temp_dir, 'model.pkl')
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                size_mb = os.path.getsize(model_path) / (1024 * 1024)
            else:
                size_mb = 0
        return size_mb
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏: {e}")
        return 0
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    try:
        if model_type in ['cnn', 'neural_network']:
            model.save(filepath)
        elif model_type == 'knn':
            with open(filepath, 'wb') as f:
                pickle.dump(model, f)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω
        if os.path.exists(filepath):
            actual_size = os.path.getsize(filepath) / (1024 * 1024)
            return True, f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({actual_size:.1f}MB)"
        else:
            return False, "–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω"
    except Exception as e:
        return False, f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    db_file = 'database/experiments.db'
    db_exists = os.path.exists(db_file)
    
    conn = sqlite3.connect(db_file)
    c = conn.cursor()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    c.execute('''
        CREATE TABLE IF NOT EXISTS experiments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            experiment_name TEXT NOT NULL,
            model_type TEXT NOT NULL,
            model_filename TEXT NOT NULL,
            parameters TEXT NOT NULL,
            dataset_info TEXT NOT NULL,
            accuracy REAL NOT NULL,
            precision REAL NOT NULL,
            recall REAL NOT NULL,
            f1_score REAL NOT NULL,
            training_time REAL NOT NULL,
            created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            enhancement_applied BOOLEAN NOT NULL,
            augmentation_type TEXT NOT NULL,
            sample_size INTEGER NOT NULL,
            model_size_mb REAL,
            save_status TEXT NOT NULL
        )
    ''')
    
    conn.commit()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —Ç–æ–ª—å–∫–æ —á—Ç–æ —Å–æ–∑–¥–∞–Ω–∞
    c.execute("SELECT COUNT(*) FROM experiments")
    record_count = c.fetchone()[0]
    
    conn.close()
    
    return db_exists, record_count

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
db_existed, initial_records = init_database()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session state
if 'experiments' not in st.session_state:
    st.session_state.experiments = []
if 'enhancement_applied' not in st.session_state:
    st.session_state.enhancement_applied = False
if 'x_train_enhanced' not in st.session_state:
    st.session_state.x_train_enhanced = None
if 'x_test_enhanced' not in st.session_state:
    st.session_state.x_test_enhanced = None

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö CIFAR-10 —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
@st.cache_data
def load_data():
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    return (x_train, y_train), (x_test, y_test)

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
def enhance_image(image):
    """–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if isinstance(image, np.ndarray):
        image = Image.fromarray((image * 255).astype(np.uint8))
    
    enhancer = ImageEnhance.Contrast(image)
    image = enhancer.enhance(1.2) # –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç –Ω–∞ 20%
    
    enhancer = ImageEnhance.Sharpness(image)
    image = enhancer.enhance(1.3) # –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–µ–∑–∫–æ—Å—Ç—å –Ω–∞ 30%
    
    enhancer = ImageEnhance.Brightness(image)
    image = enhancer.enhance(1.1) # –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —è—Ä–∫–æ—Å—Ç—å –Ω–∞ 10%
    
    return np.array(image) / 255.0

def get_augmentation_parameters(augmentation_type='basic'):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è Keras"""
    if augmentation_type == 'none':
        return None
    
    params = {
        'rotation_range': 15 if augmentation_type == 'basic' else 20,
        'width_shift_range': 0.1 if augmentation_type == 'basic' else 0.15,
        'height_shift_range': 0.1 if augmentation_type == 'basic' else 0.15,
        'horizontal_flip': True,
        'zoom_range': 0.1 if augmentation_type == 'basic' else 0.2,
        'fill_mode': 'nearest'
    }
    return params

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K (–º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è)
def find_optimal_k(x_train_flat, y_train, max_k=15):
    """–ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è"""
    st.info("–ò—â–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ K –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è...")
    
    k_range = range(1, max_k + 1, 2)
    k_scores = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, k in enumerate(k_range):
        status_text.text(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º K={k}...")
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, x_train_flat[:2000], y_train[:2000], cv=3, scoring='accuracy')
        k_scores.append(scores.mean())
        progress_bar.progress((i + 1) / len(k_range))
    
    differences = [k_scores[i] - k_scores[i-1] for i in range(1, len(k_scores))]
    optimal_k_index = differences.index(max(differences)) + 1
    optimal_k = k_range[optimal_k_index]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(k_range, k_scores, 'bo-', alpha=0.7)
    ax.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7, label=f'–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ K={optimal_k}')
    ax.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π (K)')
    ax.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    ax.set_title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    return optimal_k, fig

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
def recommend_nn_parameters(sample_size):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏"""
    if sample_size <= 5000:
        return {'epochs': 10, 'batch_size': 32, 'units1': 64, 'units2': 32, 'dropout': 0.3}
    elif sample_size <= 15000:
        return {'epochs': 15, 'batch_size': 64, 'units1': 128, 'units2': 64, 'dropout': 0.4}
    else:
        return {'epochs': 20, 'batch_size': 128, 'units1': 256, 'units2': 128, 'dropout': 0.5}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CNN
def recommend_cnn_parameters(sample_size):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CNN –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏"""
    if sample_size <= 5000:
        return {'epochs': 10, 'batch_size': 32, 'filters1': 32, 'filters2': 64, 'dense_units': 64}
    elif sample_size <= 15000:
        return {'epochs': 15, 'batch_size': 64, 'filters1': 64, 'filters2': 128, 'dense_units': 128}
    else:
        return {'epochs': 20, 'batch_size': 128, 'filters1': 128, 'filters2': 256, 'dense_units': 256}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
def save_experiment_to_db(experiment_data):
    if experiment_data['model_type'] == 'K-NN':
        experiment_data['augmentation_type'] = 'none'

    conn = sqlite3.connect('database/experiments.db')
    c = conn.cursor()
    
    c.execute('''
        INSERT INTO experiments 
        (experiment_name, model_type, model_filename, parameters, dataset_info, 
         accuracy, precision, recall, f1_score, training_time, 
         enhancement_applied, augmentation_type, sample_size, model_size_mb, save_status)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        experiment_data['experiment_name'],
        experiment_data['model_type'],
        experiment_data['model_filename'],
        experiment_data['parameters'],
        experiment_data['dataset_info'],
        experiment_data['accuracy'],
        experiment_data['precision'],
        experiment_data['recall'],
        experiment_data['f1_score'],
        experiment_data['training_time'],
        experiment_data['enhancement_applied'],
        experiment_data['augmentation_type'],
        experiment_data['sample_size'],
        experiment_data.get('model_size_mb', 0),
        experiment_data.get('save_status', 'unknown')
    ))
    
    conn.commit()
    conn.close()

# –í—ã–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã –∏–∑ –ë–î –≤ csv
def export_experiments_to_csv():
    conn = sqlite3.connect('database/experiments.db')
    df = pd.read_sql_query('SELECT * FROM experiments', conn)
    conn.close()
    return df

# –°–æ—Ö—Ä–∞–Ω–ï–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ —Ä–∞–∑–º–µ—Ä–∞
def save_model_with_check(model, filepath, model_type, experiment_name):
    try:
        settings = SAVE_SETTINGS.get(model_type, {'enabled': False, 'max_size_mb': 0})
        
        if not settings['enabled']:
            return False, f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π {model_type} –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
        
        # –î–ª—è KNN –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–∞–∑–º–µ—Ä–∞ (–æ–Ω–∏ –æ–±—ã—á–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ)
        if model_type == 'knn':
            try:
                with open(filepath, 'wb') as f:
                    pickle.dump(model, f)
                actual_size = os.path.getsize(filepath) / (1024 * 1024)
                return True, f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({actual_size:.1f}MB)"
            except Exception as e:
                return False, f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}"
        
        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
        size_mb = get_model_size(model, model_type)
        max_size_mb = settings['max_size_mb']
        
        if size_mb > max_size_mb:
            return False, f"–ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è: {size_mb:.1f}MB > {max_size_mb}MB"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        try:
            if model_type in ['cnn', 'neural_network']:
                model.save(filepath)
            elif model_type == 'knn':
                with open(filepath, 'wb') as f:
                    pickle.dump(model, f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω
            if os.path.exists(filepath):
                actual_size = os.path.getsize(filepath) / (1024 * 1024)
                return True, f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({actual_size:.1f}MB)"
            else:
                return False, "–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω"
                
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}"
    
    except Exception as e:
        # –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–´–ô –≤–æ–∑–≤—Ä–∞—Ç –ø—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ
        return False, f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def load_experiments_from_db():
    conn = sqlite3.connect('database/experiments.db')
    df = pd.read_sql_query('SELECT * FROM experiments ORDER BY created_date DESC', conn)
    conn.close()
    return df

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
def create_extended_plots(y_true, y_pred, y_pred_proba=None, class_names=None):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–∏"""
    plots = {}
    
    fig1, ax1 = plt.subplots(figsize=(10, 8))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    ax1.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    ax1.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    ax1.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
    plots['confusion_matrix'] = fig1
    
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
    metrics_df = pd.DataFrame(report).transpose()
    
    fig2, ax2 = plt.subplots(figsize=(12, 6))
    class_metrics = metrics_df.iloc[:-3, :3]
    x = range(len(class_metrics))
    width = 0.25
    
    ax2.bar([i - width for i in x], class_metrics['precision'], width, label='Precision', alpha=0.8)
    ax2.bar(x, class_metrics['recall'], width, label='Recall', alpha=0.8)
    ax2.bar([i + width for i in x], class_metrics['f1-score'], width, label='F1-Score', alpha=0.8)
    
    ax2.set_xlabel('–ö–ª–∞—Å—Å—ã')
    ax2.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫')
    ax2.set_title('–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º')
    ax2.set_xticks(x)
    ax2.set_xticklabels(class_metrics.index, rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    plots['class_metrics'] = fig2
    
    if y_pred_proba is not None:
        y_true_bin = label_binarize(y_true, classes=range(len(class_names)))
        n_classes = len(class_names)
        
        fig3, ax3 = plt.subplots(figsize=(10, 8))
        colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))
        
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])
            roc_auc = auc(fpr, tpr)
            ax3.plot(fpr, tpr, color=colors[i], lw=2,
                    label=f'{class_names[i]} (AUC = {roc_auc:.2f})')
        
        ax3.plot([0, 1], [0, 1], 'k--', lw=2)
        ax3.set_xlim([0.0, 1.0])
        ax3.set_ylim([0.0, 1.05])
        ax3.set_xlabel('False Positive Rate')
        ax3.set_ylabel('True Positive Rate')
        ax3.set_title('ROC-–∫—Ä–∏–≤—ã–µ –ø–æ –∫–ª–∞—Å—Å–∞–º')
        ax3.legend(loc="lower right")
        ax3.grid(True, alpha=0.3)
        plots['roc_curves'] = fig3
    
    return plots, metrics_df

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–µ–∫—Ç–∞
def check_project_structure():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    folders = ['models', 'experiments', 'database']
    status = {}
    
    for folder in folders:
        exists = os.path.exists(folder)
        is_dir = os.path.isdir(folder) if exists else False
        file_count = 0
        
        if exists and is_dir:
            try:
                file_count = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])
            except:
                file_count = 0
        
        status[folder] = {
            'exists': exists,
            'is_directory': is_dir,
            'file_count': file_count
        }
    
    return status

# –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
class_names = ['—Å–∞–º–æ–ª—ë—Ç', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–ø—Ç–∏—Ü–∞', '–∫–æ—à–∫–∞', '–æ–ª–µ–Ω—å', 
               '—Å–æ–±–∞–∫–∞', '–ª—è–≥—É—à–∫–∞', '–ª–æ—à–∞–¥—å', '–∫–æ—Ä–∞–±–ª—å', '–≥—Ä—É–∑–æ–≤–∏–∫']

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üñºÔ∏è –£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CIFAR-10")
st.write("–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
if created:
    st.sidebar.success(f"‚úÖ –°–æ–∑–¥–∞–Ω—ã –ø–∞–ø–∫–∏: {', '.join(created)}")
if existing:
    st.sidebar.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ: {', '.join(existing)}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
with st.spinner('–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ CIFAR-10...'):
    (x_train, y_train), (x_test, y_test) = load_data()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")

SAVE_SETTINGS['knn']['enabled'] = st.sidebar.checkbox("–°–æ—Ö—Ä–∞–Ω—è—Ç—å KNN –º–æ–¥–µ–ª–∏", value=True)
SAVE_SETTINGS['neural_network']['enabled'] = st.sidebar.checkbox("–°–æ—Ö—Ä–∞–Ω—è—Ç—å Neural Network –º–æ–¥–µ–ª–∏", value=True)
SAVE_SETTINGS['cnn']['enabled'] = st.sidebar.checkbox("–°–æ—Ö—Ä–∞–Ω—è—Ç—å CNN –º–æ–¥–µ–ª–∏", value=True)

SAVE_SETTINGS['neural_network']['max_size_mb'] = st.sidebar.slider(
    "–ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä NN –º–æ–¥–µ–ª–µ–π (MB)", 10, 100, 50
)
SAVE_SETTINGS['cnn']['max_size_mb'] = st.sidebar.slider(
    "–ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä CNN –º–æ–¥–µ–ª–µ–π (MB)", 50, 500, 200
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")
enhance_quality = st.sidebar.checkbox("–£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", value=False)

# –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
if enhance_quality and not st.session_state.enhancement_applied:
    if st.sidebar.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º"):
        with st.spinner('–ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º...'):
            x_train_enhanced = x_train.copy()
            x_test_enhanced = x_test.copy()
            
            st.info("üîß –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            x_train_enhanced = np.array([enhance_image(img) for img in x_train_enhanced])
            x_test_enhanced = np.array([enhance_image(img) for img in x_test_enhanced])
            
            st.session_state.x_train_enhanced = x_train_enhanced
            st.session_state.x_test_enhanced = x_test_enhanced
            st.session_state.enhancement_applied = True
            
            st.sidebar.success("‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞")

st.sidebar.subheader("–°—Ç–∞—Ç—É—Å —É–ª—É—á—à–µ–Ω–∏—è")
if st.session_state.enhancement_applied:
    st.sidebar.success("‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
else:
    st.sidebar.info("‚ÑπÔ∏è –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
enable_augmentation = st.sidebar.checkbox("–í–∫–ª—é—á–∏—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö", value=False)

if enable_augmentation:
    augmentation_type = st.sidebar.selectbox("–¢–∏–ø –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", ['basic', 'advanced'], index=0)
    
    with st.sidebar.expander("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"):
        aug_params = get_augmentation_parameters(augmentation_type)
        st.write("**–¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
        for param, value in aug_params.items():
            st.write(f"- {param}: {value}")
else:
    augmentation_type = 'none'

st.sidebar.subheader("–°—Ç–∞—Ç—É—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
if enable_augmentation:
    st.sidebar.success(f"‚úÖ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {augmentation_type}")
else:
    st.sidebar.info("‚ÑπÔ∏è –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")    

# –°–±—Ä–æ—Å —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫
if st.session_state.enhancement_applied and not enhance_quality:
    st.session_state.enhancement_applied = False
    st.session_state.x_train_enhanced = None
    st.session_state.x_test_enhanced = None
    st.sidebar.info("–£–ª—É—á—à–µ–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω—ã")

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏
st.subheader("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–±–æ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

col1, col2 = st.columns(2)

with col1:
    sample_size = st.slider(
        '–†–∞–∑–º–µ—Ä –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ô –≤—ã–±–æ—Ä–∫–∏', 
        1000, 50000, 10000,
        help="CIFAR-10 —Å–æ–¥–µ—Ä–∂–∏—Ç 50,000 —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
    )

with col2:
    test_sample_size = st.slider(
        '–†–∞–∑–º–µ—Ä –¢–ï–°–¢–û–í–û–ô –≤—ã–±–æ—Ä–∫–∏', 
        1000, 10000, 2000,
        help="CIFAR-10 —Å–æ–¥–µ—Ä–∂–∏—Ç 10,000 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
    )

st.info("""
**üìù –ü–æ—è—Å–Ω–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º –≤—ã–±–æ—Ä–∫–∏:**
- **–ü–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç CIFAR-10**: 60,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (50,000 —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö + 10,000 —Ç–µ—Å—Ç–æ–≤—ã—Ö)
- **–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –¥–æ 50,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π  
- **–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ**: –¥–æ 10,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
""")    

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
if st.session_state.enhancement_applied:
    x_train_small = st.session_state.x_train_enhanced[:sample_size]
    x_test_small = st.session_state.x_test_enhanced[:test_sample_size]
    current_data_type = "—É–ª—É—á—à–µ–Ω–Ω—ã–µ"
else:
    x_train_small = x_train[:sample_size]
    x_test_small = x_test[:test_sample_size]
    current_data_type = "–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ"

y_train_small = y_train[:sample_size].flatten()
y_test_small = y_test[:test_sample_size].flatten()

st.success(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã! {len(x_train_small)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∏ {len(x_test_small)} —Ç–µ—Å—Ç–æ–≤—ã—Ö {current_data_type} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
st.sidebar.title("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
section = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª:", 
                           ["–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "K-Nearest Neighbors", "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", 
                            "–°–≤—ë—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å (CNN)", "–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"])

# –†–∞–∑–¥–µ–ª 1: –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö
if section == "–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö":
    st.header("üìä –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö CIFAR-10")
    
    # –°—Ç–∞—Ç—É—Å —É–ª—É—á—à–µ–Ω–∏—è
    if st.session_state.enhancement_applied:
        st.success("‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    else:
        st.info("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    # –°—Ç–∞—Ç—É—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    if enable_augmentation:
        st.info(f"üéØ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞: {augmentation_type}")
    else:
        st.info("üéØ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if st.session_state.enhancement_applied:
        st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –æ—Ä–∏–≥–∏–Ω–∞–ª vs —É–ª—É—á—à–µ–Ω–Ω–æ–µ")
        fig, axes = plt.subplots(2, 5, figsize=(15, 6))
        for i in range(5):
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            axes[0, i].imshow(x_train[i])
            axes[0, i].set_title(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {class_names[y_train[i][0]]}")
            axes[0, i].axis('off')
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            axes[1, i].imshow(st.session_state.x_train_enhanced[i])
            axes[1, i].set_title(f"–£–ª—É—á—à–µ–Ω–Ω–æ–µ: {class_names[y_train[i][0]]}")
            axes[1, i].axis('off')
        st.pyplot(fig)
    
    # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    st.subheader("–ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    max_safe_index = min(sample_size, len(x_train_small))
    
    if 'example_indices' not in st.session_state:
        st.session_state.example_indices = []
        for class_id in range(10):
            # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            class_indices = np.where(y_train[:max_safe_index] == class_id)[0]
            if len(class_indices) > 0:
                st.session_state.example_indices.append(np.random.choice(class_indices))
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –≤—ã–±–æ—Ä–∫–µ, –±–µ—Ä–µ–º –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                class_indices_full = np.where(y_train == class_id)[0]
                if len(class_indices_full) > 0:
                    safe_index = min(np.random.choice(class_indices_full), max_safe_index - 1)
                    st.session_state.example_indices.append(safe_index)

    if st.button('üîÑ –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã'):
        st.session_state.example_indices = []
        for class_id in range(10):
            # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            class_indices = np.where(y_train[:max_safe_index] == class_id)[0]
            if len(class_indices) > 0:
                st.session_state.example_indices.append(np.random.choice(class_indices))
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –≤—ã–±–æ—Ä–∫–µ, –±–µ—Ä–µ–º –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                class_indices_full = np.where(y_train == class_id)[0]
                if len(class_indices_full) > 0:
                    safe_index = min(np.random.choice(class_indices_full), max_safe_index - 1)
                    st.session_state.example_indices.append(safe_index)

    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    for i, idx in enumerate(st.session_state.example_indices):
        ax = axes[i//5, i%5]
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        if idx < len(x_train_small):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (x_train_small)
            ax.imshow(x_train_small[idx])
            ax.set_title(f"{class_names[y_train_small[idx]]}")
        else:
            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            ax.text(0.5, 0.5, f"–ò–Ω–¥–µ–∫—Å {idx}\n–≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞", 
                   ha='center', va='center', transform=ax.transAxes, fontsize=10)
            ax.set_title("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞")
        
        ax.axis('off')
    
    st.pyplot(fig)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤
    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")
    train_counts = [np.sum(y_train_small == i) for i in range(10)]
    test_counts = [np.sum(y_test_small == i) for i in range(10)]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    ax1.bar(class_names, train_counts)
    ax1.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ")
    ax1.tick_params(axis='x', rotation=45)
    ax2.bar(class_names, test_counts)
    ax2.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ")
    ax2.tick_params(axis='x', rotation=45)
    st.pyplot(fig)

# –†–∞–∑–¥–µ–ª 2: K-Nearest Neighbors (–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å)
elif section == "K-Nearest Neighbors":
    st.header("üßÆ K-Nearest Neighbors (K-NN)")
    st.write("""
    –ù–∞—á–Ω—ë–º —Å –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏! K-NN –∏—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
    –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ "—Å–æ—Å–µ–¥–µ–π".
    """)
    
    st.info("‚ÑπÔ∏è –î–ª—è K-NN –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è - –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")

    # –ü–æ–ª–µ –¥–ª—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_timestamp = int(time.time())
    experiment_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", f"KNN_Experiment_{experiment_timestamp}")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K
    st.subheader("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    if st.button("–ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ K"):
        x_train_flat = x_train_small.reshape(x_train_small.shape[0], -1)
        optimal_k, elbow_plot = find_optimal_k(x_train_flat, y_train_small)
        st.session_state.optimal_k = optimal_k
        st.pyplot(elbow_plot)
        st.success(f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ K: {optimal_k}")

    if 'knn_initialized' not in st.session_state:
        st.session_state.knn_initialized = True
    
    default_k = st.session_state.get('optimal_k', 5)
    k_value = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π (K)", 1, 15, default_k, key="knn_unique_slider")    

    if st.button("–û–±—É—á–∏—Ç—å K-NN –º–æ–¥–µ–ª—å"):
        with st.spinner('–û–±—É—á–∞–µ–º K-NN... —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç'):
            start_time = time.time()    

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è K-NN (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä—ã)
            x_train_flat = x_train_small.reshape(x_train_small.shape[0], -1)
            x_test_flat = x_test_small.reshape(x_test_small.shape[0], -1)
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            knn = KNeighborsClassifier(n_neighbors=k_value)
            knn.fit(x_train_flat, y_train_small)
       
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            y_pred = knn.predict(x_test_flat)
            y_true = y_test_small
            
            end_time = time.time()
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            f1 = f1_score(y_true, y_pred, average='weighted')
            
            st.success(f"K-NN –æ–±—É—á–µ–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Accuracy", f"{accuracy:.2%}")
            with col2:
                st.metric("Precision", f"{precision:.2%}")
            with col3:
                st.metric("Recall", f"{recall:.2%}")
            with col4:
                st.metric("F1-Score", f"{f1:.2%}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            model_filename = f"models/knn_model_{experiment_timestamp}.pkl"
            result = save_model_with_check(knn, model_filename, 'knn', experiment_name)
            if result is None:
                save_success = False
                save_message = "–§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Ä–Ω—É–ª–∞ None"
            else:
                save_success, save_message = result
            
            model_size_mb = get_model_size(knn, 'knn')
            
            if save_success:
                st.success(f"‚úÖ {save_message}")
            else:
                st.warning(f"‚ö†Ô∏è {save_message}")
                model_filename = "not_saved"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            experiment_data = {
                'experiment_name': experiment_name,
                'model_type': 'K-NN',
                'model_filename': model_filename,
                'parameters': json.dumps({
                    'k_value': k_value,
                    'algorithm': 'auto',
                    'weights': 'uniform'
                }),
                'dataset_info': json.dumps({
                    'sample_size': sample_size,
                    'enhancement_applied': st.session_state.enhancement_applied,
                    'augmentation_type': augmentation_type,
                    'data_type': current_data_type
                }),
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'training_time': end_time - start_time,
                'enhancement_applied': st.session_state.enhancement_applied,
                'augmentation_type': augmentation_type,
                'sample_size': sample_size,
                'model_size_mb': model_size_mb,
                'save_status': 'success' if save_success else 'failed'
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            save_experiment_to_db(experiment_data)
            st.success(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç '{experiment_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            st.session_state.knn_model = knn
            st.session_state.knn_accuracy = accuracy
            
            # –ü–æ–∫–∞–∂–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            st.subheader("–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
            fig, axes = plt.subplots(2, 5, figsize=(15, 6))
            for i in range(min(10, len(y_true))):
                ax = axes[i//5, i%5]
                ax.imshow(x_test_small[i])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                true_label = class_names[y_true[i]]
                pred_label = class_names[y_pred[i]]
                color = 'green' if y_true[i] == y_pred[i] else 'red'
                ax.set_title(f"–ò—Å—Ç–∏–Ω–æ: {true_label}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {pred_label}", color=color)
                ax.axis('off')
            st.pyplot(fig)

# –†–∞–∑–¥–µ–ª 3: –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
elif section == "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å":
    st.header("üß† –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å")
    
    # –ü–æ–ª–µ –¥–ª—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_timestamp = int(time.time())
    experiment_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", f"NN_Experiment_{experiment_timestamp}")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    recommended_params = recommend_nn_parameters(sample_size)
    
    st.subheader("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    st.write(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {sample_size} ‚Üí –≠–ø–æ—Ö–∏: {recommended_params['epochs']}, "
             f"–ë–∞—Ç—á: {recommended_params['batch_size']}, "
             f"–ù–µ–π—Ä–æ–Ω—ã: {recommended_params['units1']}/{recommended_params['units2']}")
    
    epochs = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö", 1, 30, recommended_params['epochs'])
    batch_size = st.slider("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞", 32, 256, recommended_params['batch_size'])
    units1 = st.slider("–ù–µ–π—Ä–æ–Ω—ã –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ", 32, 512, recommended_params['units1'])
    units2 = st.slider("–ù–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ç–æ—Ä–æ–º —Å–ª–æ–µ", 16, 256, recommended_params['units2'])
    dropout_rate = st.slider("Dropout rate", 0.1, 0.7, recommended_params['dropout'])
    
    if st.button("–û–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å"):
        with st.spinner('–û–±—É—á–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å...'):
            start_time = time.time()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            x_train_flat = x_train_small.reshape(x_train_small.shape[0], -1)
            x_test_flat = x_test_small.reshape(x_test_small.shape[0], -1)
            
            y_train_categorical = to_categorical(y_train_small, 10)
            y_test_categorical = to_categorical(y_test_small, 10)
            
            # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
            model = Sequential([
                Dense(units1, activation='relu', input_shape=(3072,)),
                BatchNormalization(),
                Dropout(dropout_rate),
                Dense(units2, activation='relu'),
                BatchNormalization(),
                Dropout(dropout_rate * 0.8),
                Dense(10, activation='softmax')
            ])
            
            model.compile(optimizer='adam',
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])
            
            # –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            aug_params = get_augmentation_parameters(augmentation_type)
            
            if enable_augmentation and aug_params is not None:
                # –î–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π —Å–µ—Ç–∏ –Ω—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                # –ü–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞—Ç–Ω–æ
                x_train_reshaped = x_train_small.reshape(x_train_small.shape[0], 32, 32, 3)
                
                datagen = ImageDataGenerator(**aug_params)
                
                # –û–±—É—á–∞–µ–º —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
                history = model.fit(
                    datagen.flow(x_train_reshaped, y_train_categorical, batch_size=batch_size),
                    steps_per_epoch=len(x_train_reshaped) // batch_size,
                    epochs=epochs,
                    validation_data=(x_test_flat, y_test_categorical),
                    verbose=0
                )
                st.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {augmentation_type}")
            else:
                # –û–±—É—á–∞–µ–º –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                history = model.fit(
                    x_train_flat, y_train_categorical,
                    epochs=epochs, batch_size=batch_size,
                    validation_data=(x_test_flat, y_test_categorical),
                    verbose=0
                )
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            test_loss, test_accuracy = model.evaluate(x_test_flat, y_test_categorical, verbose=0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
            y_pred_proba = model.predict(x_test_flat, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            y_true = y_test_small
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            f1 = f1_score(y_true, y_pred, average='weighted')
            
            end_time = time.time()
            
            st.success(f"–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Accuracy", f"{test_accuracy:.2%}")
            with col2:
                st.metric("Precision", f"{precision:.2%}")
            with col3:
                st.metric("Recall", f"{recall:.2%}")
            with col4:
                st.metric("F1-Score", f"{f1:.2%}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
            model_filename = f"models/nn_model_{experiment_timestamp}.h5"
            save_success, save_message = save_model_with_check(
                model, model_filename, 'neural_network', experiment_name
            )
            
            model_size_mb = get_model_size(model, 'neural_network')
            
            if save_success:
                st.success(f"‚úÖ {save_message}")
            else:
                st.warning(f"‚ö†Ô∏è {save_message}")
                model_filename = "not_saved"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            experiment_data = {
                'experiment_name': experiment_name,
                'model_type': 'Neural Network',
                'model_filename': model_filename,
                'parameters': json.dumps({
                    'epochs': epochs,
                    'batch_size': batch_size,
                    'units1': units1,
                    'units2': units2,
                    'dropout_rate': dropout_rate,
                    'optimizer': 'adam',
                    'augmentation': augmentation_type
                }),
                'dataset_info': json.dumps({
                    'sample_size': sample_size,
                    'enhancement_applied': st.session_state.enhancement_applied,
                    'augmentation_type': augmentation_type,
                    'data_type': current_data_type
                }),
                'accuracy': test_accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'training_time': end_time - start_time,
                'enhancement_applied': st.session_state.enhancement_applied,
                'augmentation_type': augmentation_type,
                'sample_size': sample_size,
                'model_size_mb': model_size_mb,
                'save_status': 'success' if save_success else 'failed'
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            save_experiment_to_db(experiment_data)
            st.success(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç '{experiment_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            st.session_state.nn_model = model
            st.session_state.nn_history = history
            
            # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
            st.subheader("–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            ax1.plot(history.history['accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax1.plot(history.history['val_accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
            ax1.set_xlabel('–≠–ø–æ—Ö–∞')
            ax1.legend()
            
            ax2.plot(history.history['loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax2.plot(history.history['val_loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏')
            ax2.set_xlabel('–≠–ø–æ—Ö–∞')
            ax2.legend()
            
            st.pyplot(fig)

# –†–∞–∑–¥–µ–ª 4: –°–≤—ë—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (CNN)
elif section == "–°–≤—ë—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å (CNN)":
    st.header("üéØ –°–≤—ë—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (CNN)")
    
    # –ü–æ–ª–µ –¥–ª—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_timestamp = int(time.time()) 
    experiment_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", f"CNN_Experiment_{experiment_timestamp}")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    recommended_params = recommend_cnn_parameters(sample_size)
    
    st.subheader("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    st.write(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {sample_size} ‚Üí –≠–ø–æ—Ö–∏: {recommended_params['epochs']}, "
             f"–ë–∞—Ç—á: {recommended_params['batch_size']}, "
             f"–§–∏–ª—å—Ç—Ä—ã: {recommended_params['filters1']}/{recommended_params['filters2']}")
    
    cnn_epochs = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö", 1, 30, recommended_params['epochs'])
    cnn_batch_size = st.slider("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞", 32, 256, recommended_params['batch_size'])
    filters1 = st.slider("–§–∏–ª—å—Ç—Ä—ã –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ", 16, 128, recommended_params['filters1'])
    filters2 = st.slider("–§–∏–ª—å—Ç—Ä—ã –≤–æ –≤—Ç–æ—Ä–æ–º —Å–ª–æ–µ", 32, 256, recommended_params['filters2'])
    dense_units = st.slider("–ù–µ–π—Ä–æ–Ω—ã –≤ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–º —Å–ª–æ–µ", 32, 512, recommended_params['dense_units'])
    
    if st.button("–û–±—É—á–∏—Ç—å CNN"):
        with st.spinner('–û–±—É—á–∞–µ–º —Å–≤—ë—Ä—Ç–æ—á–Ω—É—é —Å–µ—Ç—å... —ç—Ç–æ –∑–∞–π–º—ë—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è'):
            start_time = time.time()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            y_train_categorical = to_categorical(y_train_small, 10)
            y_test_categorical = to_categorical(y_test_small, 10)
            
            # –°–æ–∑–¥–∞—ë–º CNN –º–æ–¥–µ–ª—å
            model = Sequential([
                Conv2D(filters1, (3, 3), activation='relu', input_shape=(32, 32, 3)),
                BatchNormalization(),
                MaxPooling2D((2, 2)),
                Conv2D(filters2, (3, 3), activation='relu'),
                BatchNormalization(),
                MaxPooling2D((2, 2)),
                Conv2D(filters2, (3, 3), activation='relu'),
                BatchNormalization(),
                Flatten(),
                Dense(dense_units, activation='relu'),
                Dropout(0.5),
                Dense(10, activation='softmax')
            ])
            
            model.compile(optimizer='adam',
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])
            
            # –û–ë–£–ß–ï–ù–ò–ï –° –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ï–ô –ò–ó –ù–ê–°–¢–†–û–ï–ö
            aug_params = get_augmentation_parameters(augmentation_type)
            
            if enable_augmentation and aug_params is not None:
                datagen = ImageDataGenerator(**aug_params)
                
                history = model.fit(
                    datagen.flow(x_train_small, y_train_categorical, batch_size=cnn_batch_size),
                    steps_per_epoch=len(x_train_small) // cnn_batch_size,
                    epochs=cnn_epochs,
                    validation_data=(x_test_small, y_test_categorical),
                    verbose=0
                )
                st.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {augmentation_type}")
            else:
                history = model.fit(
                    x_train_small, y_train_categorical,
                    epochs=cnn_epochs, batch_size=cnn_batch_size,
                    validation_data=(x_test_small, y_test_categorical),
                    verbose=0
                )
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            test_loss, test_accuracy = model.evaluate(x_test_small, y_test_categorical, verbose=0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
            y_pred_proba = model.predict(x_test_small, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            y_true = y_test_small
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            f1 = f1_score(y_true, y_pred, average='weighted')
            
            end_time = time.time()
            
            st.success(f"CNN –æ–±—É—á–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Accuracy", f"{test_accuracy:.2%}")
            with col2:
                st.metric("Precision", f"{precision:.2%}")
            with col3:
                st.metric("Recall", f"{recall:.2%}")
            with col4:
                st.metric("F1-Score", f"{f1:.2%}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
            model_filename = f"models/cnn_model_{experiment_name}.h5"
            save_success, save_message = save_model_with_check(
                model, model_filename, 'cnn', experiment_name
            )
            
            model_size_mb = get_model_size(model, 'cnn')
            
            if save_success:
                st.success(f"‚úÖ {save_message}")
            else:
                st.warning(f"‚ö†Ô∏è {save_message}")
                model_filename = "not_saved"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            experiment_data = {
                'experiment_name': experiment_name,
                'model_type': 'CNN',
                'model_filename': model_filename,
                'parameters': json.dumps({
                    'epochs': cnn_epochs,
                    'batch_size': cnn_batch_size,
                    'filters1': filters1,
                    'filters2': filters2,
                    'dense_units': dense_units,
                    'optimizer': 'adam',
                    'augmentation': augmentation_type
                }),
                'dataset_info': json.dumps({
                    'sample_size': sample_size,
                    'enhancement_applied': st.session_state.enhancement_applied,
                    'augmentation_type': augmentation_type,
                    'data_type': current_data_type
                }),
                'accuracy': test_accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'training_time': end_time - start_time,
                'enhancement_applied': st.session_state.enhancement_applied,
                'augmentation_type': augmentation_type,
                'sample_size': sample_size,
                'model_size_mb': model_size_mb,
                'save_status': 'success' if save_success else 'failed'
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            save_experiment_to_db(experiment_data)
            st.success(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç '{experiment_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            st.session_state.cnn_model = model
            st.session_state.cnn_history = history
            
            # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
            st.subheader("–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            ax1.plot(history.history['accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax1.plot(history.history['val_accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
            ax1.set_xlabel('–≠–ø–æ—Ö–∞')
            ax1.legend()
            
            ax2.plot(history.history['loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax2.plot(history.history['val_loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏')
            ax2.set_xlabel('–≠–ø–æ—Ö–∞')
            ax2.legend()
            
            st.pyplot(fig)

# –†–∞–∑–¥–µ–ª 5: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
elif section == "–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤":
    st.header("üìà –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã
    trained_models = []
    if 'knn_model' in st.session_state:
        trained_models.append('K-NN')
    if 'nn_model' in st.session_state:
        trained_models.append('Neural Network')
    if 'cnn_model' in st.session_state:
        trained_models.append('CNN')
    
    if not trained_models:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö.")
        st.info("""
        **–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:**
        - K-Nearest Neighbors (K-NN) - –±—ã—Å—Ç—Ä–∞—è –∏ –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å
        - Neural Network - –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å  
        - CNN - —Å–≤—ë—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (–ª—É—á—à–∞—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        """)
    else:
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        selected_model = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:", trained_models)
        
        if selected_model == 'K-NN':
            # –ê–Ω–∞–ª–∏–∑ –¥–ª—è K-NN
            knn = st.session_state.knn_model
            x_test_flat = x_test_small.reshape(x_test_small.shape[0], -1)
            y_pred = knn.predict(x_test_flat)
            y_true = y_test_small
            y_pred_proba = None
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å K-NN
            accuracy = st.session_state.get('knn_accuracy', accuracy_score(y_true, y_pred))
            st.metric("–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏", f"{accuracy:.2%}")
            
        elif selected_model == 'Neural Network':
            # –ê–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            model = st.session_state.nn_model
            history = st.session_state.nn_history
            x_test_flat = x_test_small.reshape(x_test_small.shape[0], -1)
            y_pred_proba = model.predict(x_test_flat, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            y_true = y_test_small
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
            st.subheader("–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            ax1.plot(history.history['accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax1.plot(history.history['val_accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
            ax1.set_xlabel('–≠–ø–æ—Ö–∞')
            ax1.legend()
            
            ax2.plot(history.history['loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax2.plot(history.history['val_loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏')
            ax2.set_xlabel('–≠–ø–æ—Ö–∞')
            ax2.legend()
            st.pyplot(fig)
            
        elif selected_model == 'CNN':
            # –ê–Ω–∞–ª–∏–∑ –¥–ª—è CNN
            model = st.session_state.cnn_model
            history = st.session_state.cnn_history
            y_pred_proba = model.predict(x_test_small, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            y_true = y_test_small
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
            st.subheader("–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            ax1.plot(history.history['accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax1.plot(history.history['val_accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
            ax1.set_xlabel('–≠–ø–æ—Ö–∞')
            ax1.legend()
            
            ax2.plot(history.history['loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
            ax2.plot(history.history['val_loss'], label='–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏')
            ax2.set_xlabel('–≠–ø–æ—Ö–∞')
            ax2.legend()
            st.pyplot(fig)
        
        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        st.subheader(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –º–æ–¥–µ–ª–∏: {selected_model}")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
        plots, metrics_df = create_extended_plots(y_true, y_pred, y_pred_proba, class_names)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        for plot_name, plot in plots.items():
            st.subheader(plot_name.replace('_', ' ').title())
            st.pyplot(plot)
        
        # –û—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        st.subheader("–û—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        st.dataframe(metrics_df)
        
        # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        st.subheader("–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        error_indices = np.where(y_pred != y_true)[0]
        
        if len(error_indices) > 0:
            show_errors = min(10, len(error_indices))
            error_samples = np.random.choice(error_indices, show_errors, replace=False)
            
            fig, axes = plt.subplots(2, 5, figsize=(15, 6))
            axes = axes.ravel()
            
            for i, idx in enumerate(error_samples):
                if idx < len(x_test_small):  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø
                    axes[i].imshow(x_test_small[idx])
                    true_label = class_names[y_true[idx]]
                    pred_label = class_names[y_pred[idx]]
                    
                    # –î–ª—è –º–æ–¥–µ–ª–µ–π —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    confidence = None
                    if selected_model in ['Neural Network', 'CNN'] and y_pred_proba is not None:
                        confidence = np.max(y_pred_proba[idx])
                    
                    title = f"–ò—Å—Ç–∏–Ω–æ: {true_label}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {pred_label}"
                    if confidence is not None:
                        title += f"\n–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}"
                    
                    axes[i].set_title(title, color='red')
                    axes[i].axis('off')
                else:
                    axes[i].text(0.5, 0.5, "–û—à–∏–±–∫–∞\n–¥–æ—Å—Ç—É–ø–∞", 
                               ha='center', va='center', transform=axes[i].transAxes)
                    axes[i].axis('off')
            
            # –°–∫—Ä—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ subplots
            for i in range(show_errors, 10):
                axes[i].set_visible(False)
                
            st.pyplot(fig)
        else:
            st.success("–û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ù–µ—Ç –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.")

# –†–∞–∑–¥–µ–ª 6: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
elif section == "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤":
    st.header("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    experiments_df = load_experiments_from_db()
    
    if experiments_df.empty:
        st.warning("–ü–æ–∫–∞ –Ω–µ—Ç –ø—Ä–æ–≤–µ–¥—ë–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –û–±—É—á–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å.")
    else:
        st.subheader("–í—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        display_df = experiments_df.copy()
        display_df['created_date'] = pd.to_datetime(display_df['created_date']).dt.strftime('%Y-%m-%d %H:%M')
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'training_time', 'model_size_mb']
        for col in numeric_cols:
            if col in display_df.columns:
                if col == 'training_time':
                    display_df[col] = display_df[col].round(2)
                elif col == 'model_size_mb':
                    display_df[col] = display_df[col].round(1)
                else:
                    display_df[col] = (display_df[col] * 100).round(2).astype(str) + '%'
        
        st.dataframe(display_df)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        st.subheader("–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –º–æ–¥–µ–ª—è–º
        fig1, ax1 = plt.subplots(figsize=(12, 6))
        models = experiments_df['model_type'].unique()
        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
        
        for i, model in enumerate(models):
            model_data = experiments_df[experiments_df['model_type'] == model]
            ax1.scatter(model_data['created_date'], model_data['accuracy'], 
                       label=model, color=colors[i], s=100, alpha=0.7)
        
        ax1.set_xlabel('–î–∞—Ç–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞')
        ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        ax1.set_title('–î–∏–Ω–∞–º–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.tick_params(axis='x', rotation=45)
        st.pyplot(fig1)
        
        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –º–æ–¥–µ–ª—è–º")
        
        metrics_comparison = experiments_df.groupby('model_type').agg({
            'accuracy': ['mean', 'std', 'max'],
            'precision': 'mean',
            'recall': 'mean', 
            'f1_score': 'mean',
            'training_time': 'mean',
            'model_size_mb': 'mean'
        }).round(4)

        metrics_comparison.columns = [
            'accuracy_mean', 'accuracy_std', 'accuracy_max',
            'precision_mean', 'recall_mean', 'f1_mean',
            'training_time_mean', 'model_size_mb_mean'
        ]

        metrics_comparison = metrics_comparison.fillna(0)

        st.dataframe(metrics_comparison)
        
        # 3. –í–ª–∏—è–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        st.subheader("–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")
        
        enhanced_data = experiments_df[experiments_df['enhancement_applied'] == True]
        original_data = experiments_df[experiments_df['enhancement_applied'] == False]
        
        if not enhanced_data.empty and not original_data.empty:
            fig2, axes = plt.subplots(1, 2, figsize=(15, 5))
            
            # –¢–æ—á–Ω–æ—Å—Ç—å —Å —É–ª—É—á—à–µ–Ω–∏–µ–º –∏ –±–µ–∑
            categories = ['–° —É–ª—É—á—à–µ–Ω–∏–µ–º', '–ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è']
            accuracy_means = [enhanced_data['accuracy'].mean(), original_data['accuracy'].mean()]
            accuracy_stds = [enhanced_data['accuracy'].std(), original_data['accuracy'].std()]
            
            bars1 = axes[0].bar(categories, accuracy_means, yerr=accuracy_stds, capsize=5, alpha=0.7, color=['lightgreen', 'lightcoral'])
            axes[0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
            axes[0].set_title('–í–ª–∏—è–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å')
            axes[0].grid(True, alpha=0.3)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars1, accuracy_means):
                axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
            
            # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            time_means = [enhanced_data['training_time'].mean(), original_data['training_time'].mean()]
            time_stds = [enhanced_data['training_time'].std(), original_data['training_time'].std()]
            
            bars2 = axes[1].bar(categories, time_means, yerr=time_stds, capsize=5, alpha=0.7, color=['lightblue', 'orange'])
            axes[1].set_ylabel('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)')
            axes[1].set_title('–í–ª–∏—è–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
            axes[1].grid(True, alpha=0.3)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars2, time_means):
                axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           f'{value:.1f}—Å', ha='center', va='bottom')
            
            st.pyplot(fig2)
        
        # 4. –†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π
        st.subheader("–†–∞–∑–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        
        if 'model_size_mb' in experiments_df.columns:
            fig3, ax3 = plt.subplots(figsize=(10, 6))
            saved_models = experiments_df[experiments_df['save_status'] == 'success']
            
            if not saved_models.empty:
                model_sizes = saved_models.groupby('model_type')['model_size_mb'].mean()
                colors = ['lightblue', 'lightgreen', 'lightcoral']
                bars = ax3.bar(model_sizes.index, model_sizes.values, color=colors[:len(model_sizes)], alpha=0.7)
                
                ax3.set_ylabel('–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (MB)')
                ax3.set_title('–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–µ–π –ø–æ —Ç–∏–ø–∞–º')
                ax3.grid(True, alpha=0.3)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
                for bar, value in zip(bars, model_sizes.values):
                    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           f'{value:.1f}MB', ha='center', va='bottom')
                
                st.pyplot(fig3)
        
        # 5. –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É
        st.subheader("–õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É")
        best_models = experiments_df.loc[experiments_df.groupby('model_type')['accuracy'].idxmax()]
        best_display = best_models[['experiment_name', 'model_type', 'accuracy', 'precision', 'recall', 'f1_score', 'created_date', 'model_size_mb']].copy()
        best_display['accuracy'] = (best_display['accuracy'] * 100).round(2).astype(str) + '%'
        best_display['created_date'] = pd.to_datetime(best_display['created_date']).dt.strftime('%Y-%m-%d %H:%M')
        st.dataframe(best_display)
        
        # 6. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
        selected_experiment = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:", 
                                        experiments_df['experiment_name'].values)

        if selected_experiment:
            exp_data = experiments_df[experiments_df['experiment_name'] == selected_experiment].iloc[0]
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Accuracy", f"{exp_data['accuracy']:.2%}")
                st.metric("Precision", f"{exp_data['precision']:.2%}")
            with col2:
                st.metric("Recall", f"{exp_data['recall']:.2%}")
                st.metric("F1-Score", f"{exp_data['f1_score']:.2%}")
            with col3:
                st.metric("–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è", f"{exp_data['training_time']:.1f}—Å")
                if exp_data['model_size_mb']:
                    st.metric("–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏", f"{exp_data['model_size_mb']:.1f}MB")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
            st.write("**‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:**")
            st.json(json.loads(exp_data['parameters']))
            
            st.write("**üìÅ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:**")
            st.json(json.loads(exp_data['dataset_info']))
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            st.subheader("üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
            
            if exp_data['save_status'] == 'success' and exp_data['model_filename'] != 'not_saved':
                st.success(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ: `{exp_data['model_filename']}`")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                if exp_data['model_size_mb'] and exp_data['model_size_mb'] > 0:
                    st.info(f"üì¶ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: **{exp_data['model_size_mb']:.1f} MB**")
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞
                if os.path.exists(exp_data['model_filename']):
                    file_size = os.path.getsize(exp_data['model_filename']) / (1024 * 1024)
                    st.success(f"üìÅ –§–∞–π–ª –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –¥–∏—Å–∫–µ ({file_size:.1f} MB)")
                else:
                    st.warning("‚ö†Ô∏è –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –¥–∏—Å–∫–µ")
                    
            else:
                st.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞)")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ
            st.info(f"""
            **üìä –î–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:**
            - **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è**: {exp_data['created_date']}
            - **–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏**: {exp_data['sample_size']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            - **–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞**: {'‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ' if exp_data['enhancement_applied'] else '‚ùå –ù–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ'}
            - **–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è**: {exp_data['augmentation_type']}
            - **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: {exp_data['training_time']:.1f} —Å–µ–∫—É–Ω–¥
            - **–°—Ç–∞—Ç—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è**: {'‚úÖ –£—Å–ø–µ—à–Ω–æ' if exp_data['save_status'] == 'success' else '‚ùå –ù–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ'}
            """)
    
    df = export_experiments_to_csv()
    csv = df.to_csv(index=False)
    st.download_button(
        label="üì• –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (CSV)",
        data=csv,
        file_name="experiments_data.csv",
        mime="text/csv"
    )        

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
st.sidebar.markdown("---")
st.sidebar.subheader("–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
project_status = check_project_structure()
for folder, info in project_status.items():
    if info['exists'] and info['is_directory']:
        if info['file_count'] > 0:
            st.sidebar.success(f"üìÅ {folder}/ ({info['file_count']} —Ñ–∞–π–ª–æ–≤)")
        else:
            st.sidebar.info(f"üìÅ {folder}/ (–ø—É—Å—Ç–∞—è)")
    else:
        st.sidebar.error(f"‚ùå {folder}/ (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)")

# –°—Ç–∞—Ç—É—Å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
st.sidebar.write("**–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:**")
if db_existed:
    st.sidebar.info(f"üìä –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è ({initial_records} –∑–∞–ø–∏—Å–µ–π)")
else:
    st.sidebar.success("üìä –ù–æ–≤–∞—è (—Å–æ–∑–¥–∞–Ω–∞)")

# –°—Ç–∞—Ç—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
st.sidebar.write("**–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:**")
for model_type, settings in SAVE_SETTINGS.items():
    status = "‚úÖ" if settings['enabled'] else "‚ùå"
    st.sidebar.write(f"{status} {model_type}")

st.sidebar.markdown("---")
st.sidebar.subheader("–°—Ç–∞—Ç—É—Å —É–ª—É—á—à–µ–Ω–∏—è")
if st.session_state.enhancement_applied:
    st.sidebar.success("‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
else:
    st.sidebar.info("‚ÑπÔ∏è –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")

st.sidebar.subheader("–°—Ç–∞—Ç—É—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
if enable_augmentation:
    st.sidebar.success(f"‚úÖ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {augmentation_type}")
else:
    st.sidebar.info("‚ÑπÔ∏è –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")